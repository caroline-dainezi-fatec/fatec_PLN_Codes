# üéì Fatec PLN Codes

Este reposit√≥rio cont√©m os c√≥digos e materiais desenvolvidos durante as aulas de Processamento de Linguagem Natural (PLN) na **FATEC Mau√°**, sob as orienta√ß√µes do professor [Rodolfo Lovera](https://github.com/rodolfo-lovera).

## üìå Funcionalidades

Este reposit√≥rio abrange os seguintes t√≥picos no contexto de PLN:

- **Fundamentos de PLN:** Tokeniza√ß√£o, segmenta√ß√£o e sem√¢ntica;
- **Pr√©-processamento de texto:** Limpeza, normaliza√ß√£o, remo√ß√£o de stopwords, stemming e lematiza√ß√£o;
- **Representa√ß√£o de texto:** T√©cnicas de Bag of Words, TF-IDF e embeddings (Word2Vec, GloVe, FastText).

## üöÄ Tecnologias e Ferramentas

- **Linguagem:** Python
- **Bibliotecas:** spaCy, NLTK, Gensim, re e scikit-learn
- **Ambiente:** Jupyter Notebook (Google Colab)

## üóÇÔ∏è: Estrutura do Reposit√≥rio e Descri√ß√£o das Aulas

Cada pasta do reposit√≥rio representa uma aula e cont√©m um arquivo de Jupyter Notebook criado pelo Google Colab com as atividades pr√°ticas realizadas no dia. Abaixo segue um resumo dos desenvolvimentos de cada aula:

- **Aula 2 - Fundamentos da lingu√≠stica:** Fundamentos de PLN com exemplos pr√°ticos de tokeniza√ß√£o (divis√£o de texto em unidades menores), segmenta√ß√£o e sem√¢ntica. Utiliza as bibliotecas **spaCy**, **NLTK**, e **Gensim** para analisar texto em ingl√™s e portugu√™s (spaCy), tokenizar senten√ßas e palavras (NLTK) e treinar modelos Word2Vec para vetoriza√ß√£o sem√¢ntica (Gensim). Os exemplos incluem processamento de textos e constru√ß√£o de modelos sem√¢nticos simples.

- **Aula 3 - Bibliotecas e Corpora:** Uso da biblioteca **NLTK** para manipula√ß√£o de corpora (Machado e Brown), contagem e an√°lise de frequ√™ncias de palavras, remo√ß√£o de stopwords (palavras irrelevantes) e tokeniza√ß√£o de duas formas diferentes (`word_tokenize` e `TweetTokenizer`). Os exemplos no c√≥digo demonstram t√©cnicas b√°sicas para an√°lise de texto em PLN.

- **Aula 4 - Limpeza de dados textuais:** Limpeza e normaliza√ß√£o de dados textuais para remo√ß√£o de ru√≠do (elimina√ß√£o de caracteres especiais e normaliza√ß√£o de letras), tokeniza√ß√£o, filtro de stopwords e stemming e lematiza√ß√£o (redu√ß√£o e normaliza√ß√£o de palavras √†s suas formas b√°sicas). Inclui exemplos pr√°ticos de um processo completo para preparar textos em PLN utilizando essas t√©cnicas.

- **Aula 5 - Representa√ß√£o de texto:** Representa√ß√£o de texto com **Bag of Words** (BoW) utilizando `CountVectorizer` para criar uma matriz de frequ√™ncia de palavras a partir de textos, compara√ß√£o entre as representa√ß√µes BoW e TF-IDF utilizando `TfidfVectorizer` para calcular frequ√™ncias ponderadas de palavras e a combina√ß√£o do processo de limpeza de dados e a representa√ß√£o BoW para gerar uma matriz vetorizada de textos processados (aulas 4 e 5). O c√≥digo inclui exemplos pr√°ticos para cada um dos cen√°rios, utilizando as bibliotecas **scikit-learn**, **NLTK** e **re**.

- **Aula 6 - Representa√ß√£o de texto com Embeddings:** T√©cnicas de embeddings de texto com **Word2Vec**, **GloVe** e **FastText** para representar palavras de forma vetorizada e analisar rela√ß√µes sem√¢nticas. Os processos incluem o treinamento de embeddings a partir de frases simples em portugu√™s para calcular similaridades entre palavras (Word2Vec), utiliza√ß√£o de embeddings pr√©-treinados em ingl√™s para identificar a rela√ß√£o entre palavras e listar termos semanticamente pr√≥ximos (GloVe) e embeddings pr√©-treinados em portugu√™s para avaliar a similaridade entre as palavras mais pr√≥ximas (FastText).

- **Aula 7 - Modelagem de T√≥picos com Latent Dirichlet Allocation (LDA):** Utiliza√ß√£o do modelo LDA para an√°lise de t√≥picos em corpora textuais. Inclui o pr√©-processamento de textos com NLTK, como tokeniza√ß√£o, remo√ß√£o de stopwords e lematiza√ß√£o. O corpus **Reuters** √© utilizado como exemplo para treinar o modelo LDA e identificar t√≥picos ocultos nos documentos. O c√≥digo tamb√©m demonstra a cria√ß√£o de um dicion√°rio, a convers√£o dos textos para o formato Bag of Words (BoW), e o treinamento do modelo LDA com 5 t√≥picos. Al√©m disso, √© mostrada a visualiza√ß√£o interativa dos t√≥picos utilizando a biblioteca `pyLDAvis`. Um segundo exemplo usa um conjunto de dados em portugu√™s para ilustrar a cria√ß√£o de um dicion√°rio, o treinamento do modelo LDA com 3 t√≥picos e a visualiza√ß√£o dos resultados.

  *PS: Explica√ß√µes mais detalhadas sobre o c√≥digo podem ser encontradas dentro do pr√≥prio Notebook da aula correspondente.*

- **Aula 8  - Introdu√ß√£o a ML para PLN:** Aborda a classifica√ß√£o de textos utilizando m√©todos probabil√≠sticos e modelos de machine learning. O c√≥digo inclui um corpus simples com r√≥tulos de sentimento onde s√£o calculadas as probabilidades a priori e condicionais para classificar novos textos com base em palavras. Al√©m disso, s√£o treinados modelos de **Naive Bayes** e **SVM** usando a t√©cnica TF-IDF para representar os textos numericamente, e as performances dos modelos s√£o avaliadas com o relat√≥rio de classifica√ß√£o. O exemplo demonstra o uso de scikit-learn para pr√©-processamento de texto e treinamento de modelos.

  *PS: Explica√ß√µes mais detalhadas sobre o c√≥digo podem ser encontradas dentro do pr√≥prio Notebook da aula correspondente.*

## ‚öôÔ∏è Como Executar os C√≥digos do Reposit√≥rio

Recomenda-se a utiliza√ß√£o da plataforma [Google Colab](https://colab.google/) para a execu√ß√£o dos notebooks. Para executar os c√≥digos presentes no reposit√≥rio:

- Acesse um dos notebooks dentro das pastas do reposit√≥rio;
- No topo do notebook, clique em **Open in Colab**;
- Clique no bot√£o de play √† esquerda dos trechos de c√≥digo;
- Se solicitado, fazer login com uma conta do Google.

Com o login realizado, voc√™ pode executar o c√≥digo j√° escrito ou realizar altera√ß√µes tempor√°rias para testar as funcionalidades. Caso queira salvar uma c√≥pia do c√≥digo para voc√™:

- No topo superior esquerdo, clique em **Copiar para o Drive**.

Com uma c√≥pia salva no Google Drive, quaisquer altera√ß√µes que voc√™ realizar ser√£o salvas automaticamente na sua conta do Google.

üíª Desenvolvido por Caroline Dainezi.
